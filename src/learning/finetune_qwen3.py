"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è fine-tuning –º–æ–¥–µ–ª–∏ Qwen3 —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤.

–°–∫—Ä–∏–ø—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Qwen3 –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–¥–ª—è –∑–∞–¥–∞—á–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–∏–∫–∏ LoRA
(Low-Rank Adaptation) –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
"""

import os
import json
from pathlib import Path
from datetime import datetime

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model

from src.utils.config import MODEL_CACHE_DIR


def get_device():
    """
    –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: TPU ‚Üí GPU ‚Üí CPU.
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (device, device_type) –≥–¥–µ:
        - device: torch.device –æ–±—ä–µ–∫—Ç
        - device_type: —Å—Ç—Ä–æ–∫–∞ "tpu", "gpu" –∏–ª–∏ "cpu"
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ TPU
    try:
        import torch_xla.core.xla_model as xm
        device = xm.xla_device()
        num_cores = os.environ.get("TPU_NUM_CORES", "8")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º TPU (—è–¥—Ä–∞: {num_cores})")
        return device, "tpu"
    except ImportError:
        pass
    except Exception as e:
        print(f"TPU –Ω–∞–π–¥–µ–Ω, –Ω–æ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    if torch.cuda.is_available():
        print("GPU –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA")
        return torch.device("cuda"), "gpu"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU
    print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    return torch.device("cpu"), "cpu"


def get_torch_dtype(device_type):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö torch –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞.
    
    Args:
        device_type: –¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ("tpu", "gpu" –∏–ª–∏ "cpu")
        
    Returns:
        torch.dtype –¥–ª—è –º–æ–¥–µ–ª–∏
    """
    if device_type == "tpu":
        return torch.bfloat16  # TPU —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å bfloat16
    elif device_type == "gpu":
        return torch.float16  # –ù–∞ GPU –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å float16
    else:
        return torch.float32  # –ù–∞ CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32


def load_model_and_tokenizer(device, torch_dtype):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    
    Args:
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        torch_dtype: –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (tokenizer, model)
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {MODEL_CACHE_DIR}")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}, dtype: {torch_dtype}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(
            str(MODEL_CACHE_DIR),
            trust_remote_code=True
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è causal language modeling
        model = AutoModelForCausalLM.from_pretrained(
            str(MODEL_CACHE_DIR),
            torch_dtype=torch_dtype,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        model = model.to(device)
        
        print("–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        return tokenizer, model
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        print(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_CACHE_DIR}")
        raise


def setup_lora(model):
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏.
    
    Args:
        model: –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        
    Returns:
        –ú–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
    """
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA
    peft_config = LoraConfig(
        r=16,  # –†–∞–Ω–≥ –º–∞—Ç—Ä–∏—Ü (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –≤—ã—à–µ —Ç–æ—á–Ω–æ—Å—Ç—å)
        lora_alpha=32,  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É—Å–∏–ª–µ–Ω–∏—è LoRA –≤–µ—Å–æ–≤
        target_modules=["q_proj", "v_proj"],  # –¶–µ–ª–µ–≤—ã–µ —Å–ª–æ–∏ –¥–ª—è Qwen3
        lora_dropout=0.05,  # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        bias="none",  # –ù–µ –æ–±—É—á–∞–µ–º bias
        task_type="CAUSAL_LM"  # –¢–∏–ø –∑–∞–¥–∞—á–∏ - –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å
    )
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ –º–æ–¥–µ–ª–∏
    model = get_peft_model(model, peft_config)
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    model.print_trainable_parameters()
    
    print("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
    return model


def load_training_dataset(data_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ JSONL —Ñ–∞–π–ª–∞.
    
    Args:
        data_path: –ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
        
    Returns:
        Dataset –æ–±—ä–µ–∫—Ç –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ datasets
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {data_path}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º JSONL —Ñ–∞–π–ª
    dataset = load_dataset("json", data_files=str(data_path), split="train")
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
    return dataset


def tokenize_function(example, idx, tokenizer):
    """
    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –æ—Ç–¥–µ–ª—å–Ω–æ.
    
    –ö–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å (system + user + assistant) —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –∫–∞–∫ –æ–¥–∏–Ω –æ–±—É—á–∞—é—â–∏–π sample.
    –ù–∏–∫–∞–∫–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –æ–¥–∏–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –ú–∞—Å–∫–∏—Ä—É–µ—Ç system + user –≤ labels (-100), –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ assistant –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
    
    Args:
        example: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ 'mode', 'system', 'user', 'assistant'
        idx: –ò–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (input_ids, attention_mask, labels)
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º mode
    mode = example.get("mode", "unknown")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º assistant –∏–∑ dict –≤ JSON —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ —ç—Ç–æ dict
    assistant_text = example['assistant']
    if isinstance(assistant_text, dict):
        assistant_text = json.dumps(assistant_text, ensure_ascii=False, separators=(',', ':'))
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º system prompt —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã (–æ–¥–∏–Ω —Ä–∞–∑)
    system_prompt = f"–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: {mode}. {example['system']}"
    
    # –®–∞–≥ 1: –§–æ—Ä–º–∏—Ä—É–µ–º prompt_text (system + user) —Å generation prompt
    prompt_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": example['user']}
    ]
    prompt_text = tokenizer.apply_chat_template(
        prompt_messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # –®–∞–≥ 2: –§–æ—Ä–º–∏—Ä—É–µ–º full_text (system + user + assistant) –±–µ–∑ generation prompt
    full_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": example['user']},
        {"role": "assistant", "content": assistant_text}
    ]
    full_text = tokenizer.apply_chat_template(
        full_messages,
        tokenize=False,
        add_generation_prompt=False
    )
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ 100-–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–Ω–∞—á–∏–Ω–∞—è —Å –ø–µ—Ä–≤–æ–≥–æ)
    prompt_num = idx + 1
    if prompt_num == 1 or prompt_num % 100 == 0:
        print(f"\n{'='*70}")
        print(f"–ü–†–û–ú–ü–¢ #{prompt_num} (—Ä–µ–∂–∏–º: {mode})")
        print(f"{'='*70}")
        print(f"Prompt —á–∞—Å—Ç—å:\n{prompt_text}")
        print(f"\nFull text:\n{full_text}")
        print(f"{'='*70}\n")
    
    # –®–∞–≥ 3: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –ë–ï–ó truncation –∏ –ë–ï–ó padding
    tokenized = tokenizer(
        full_text,
        truncation=False,
        padding=False
    )
    
    # –®–∞–≥ 4: –°–æ–∑–¥–∞–µ–º labels - –∫–æ–ø–∏—è input_ids
    labels = tokenized["input_ids"].copy()
    
    # –®–∞–≥ 5: –ú–∞—Å–∫–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ labels (system + user)
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ prompt —á–∞—Å—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω—ã
    prompt_tokenized = tokenizer(prompt_text, add_special_tokens=False)
    prompt_len = len(prompt_tokenized["input_ids"])
    
    # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã prompt –Ω–∞ -100 (–∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è –≤ loss)
    # Assistant —Ç–æ–∫–µ–Ω—ã –æ—Å—Ç–∞—é—Ç—Å—è –Ω–µ—Ç—Ä–æ–Ω—É—Ç—ã–º–∏
    labels[:prompt_len] = [-100] * prompt_len
    
    tokenized["labels"] = labels
    
    return tokenized


def prepare_dataset(dataset, tokenizer):
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∫ –æ–±—É—á–µ–Ω–∏—é - —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã.
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç tokenize_function –∫ –∫–∞–∂–¥–æ–π –∑–∞–ø–∏—Å–∏ –æ—Ç–¥–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ dataset.map.
    –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ JSONL ‚Üí –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä.
    
    Args:
        dataset: –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –º–æ–¥–µ–ª–∏
        
    Returns:
        –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    """
    print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ –≤—Å–µ–º –ø—Ä–∏–º–µ—Ä–∞–º –æ—Ç–¥–µ–ª—å–Ω–æ
    # with_indices=True –ø–µ—Ä–µ–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ —Ñ—É–Ω–∫—Ü–∏—é
    tokenized_dataset = dataset.map(
        lambda x, idx: tokenize_function(x, idx, tokenizer),
        with_indices=True,
        remove_columns=["mode", "system", "user", "assistant"]  # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    )
    
    print("–î–∞—Ç–∞—Å–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return tokenized_dataset


def setup_training_arguments(device_type, output_dir):
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è.
    
    Args:
        device_type: –¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ("tpu", "gpu" –∏–ª–∏ "cpu")
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        
    Returns:
        TrainingArguments –æ–±—ä–µ–∫—Ç
    """
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=1,  # –û–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∑–∞ —Ä–∞–∑
        gradient_accumulation_steps=8,  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        learning_rate=2e-5,  # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        num_train_epochs=3,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        logging_dir=str(output_dir / "logs"),
        logging_steps=10,  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 10 —à–∞–≥–æ–≤
        save_strategy="steps",  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö N —à–∞–≥–æ–≤
        save_steps=5,  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
        save_total_limit=2,  # –•—Ä–∞–Ω–∏—Ç—å 5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        report_to="none",  # –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –≤ wandb/tensorboard
        remove_unused_columns=False,
        fp16=True if device_type == "gpu" else False,  # Mixed precision –¥–ª—è GPU
        bf16=True if device_type == "tpu" else False,  # bfloat16 –¥–ª—è TPU
        warmup_steps=100,  # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ learning rate
        weight_decay=0.01,  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        dataloader_num_workers=0,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        resume_from_checkpoint=True,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª—è—Ç—å —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    )
    
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã:")
    print(f"  - Batch size: 1 (–æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∑–∞ —Ä–∞–∑)")
    print(f"  - Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"  - Learning rate: {training_args.learning_rate}")
    print(f"  - Epochs: {training_args.num_train_epochs}")
    print(f"  - –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: –∫–∞–∂–¥—ã–µ {training_args.save_steps} —à–∞–≥–æ–≤")
    print(f"  - –•—Ä–∞–Ω–∏—Ç—Å—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {training_args.save_total_limit}")
    print(f"  - –ê–≤—Ç–æ–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: –≤–∫–ª—é—á–µ–Ω–æ")
    
    return training_args


def train_model(model, tokenizer, tokenized_dataset, training_args):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
    
    Args:
        model: –ú–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        tokenized_dataset: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        training_args: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer...")
    
    # Data collator –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –ø–∞–¥–¥–∏–Ω–≥–∞
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º Masked Language Modeling
    )
    
    # –°–æ–∑–¥–∞–µ–º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )
    
    print("=" * 70)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 70)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    trainer.train()
    
    print("=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 70)
    
    return model


def save_lora_adapters(model, tokenizer, output_path):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.
    
    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
    """
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –≤: {output_path}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_path.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (–Ω–µ –≤—Å—é –º–æ–¥–µ–ª—å)
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    print("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞.
    """
    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
    start_time = datetime.now()
    
    print("=" * 70)
    print("FINE-TUNING –ú–û–î–ï–õ–ò QWEN –° LORA")
    print("=" * 70)
    print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    script_dir = Path(__file__).parent
    data_path = script_dir / "qwen3_structured_train.jsonl"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –º–æ–¥–µ–ª—å—é
    model_base_dir = MODEL_CACHE_DIR
    output_dir = model_base_dir / "checkpoints"
    adapters_dir = model_base_dir / "lora_adapters"
    
    print(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_CACHE_DIR}")
    print(f"–î–∞–Ω–Ω—ã–µ: {data_path}")
    print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {output_dir}")
    print(f"–ê–¥–∞–ø—Ç–µ—Ä—ã: {adapters_dir}")
    
    print("\n" + "=" * 70)
    print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ß–ï–ö–ü–û–ò–ù–¢–ê–• –ò –í–û–ó–û–ë–ù–û–í–õ–ï–ù–ò–ò –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 70)
    print("‚Ä¢ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é checkpoints/")
    print("‚Ä¢ –•—Ä–∞–Ω—è—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞")
    print("‚Ä¢ –ü—Ä–∏ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è (Ctrl+C –∏–ª–∏ –≤—ã–∫–ª—é—á–µ–Ω–∏–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞):")
    print("  - –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞: python src/learning/finetune_qwen3.py")
    print("  - –û–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
    print("‚Ä¢ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—Ç: –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏, optimizer state, scheduler state")
    print("=" * 70 + "\n")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    print("=" * 70)
    print("–°–¢–ê–¢–£–° –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 70)
    
    if output_dir.exists():
        # –ò—â–µ–º –≤—Å–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
        checkpoints = sorted([d for d in output_dir.glob("checkpoint-*") if d.is_dir()])
        
        if checkpoints:
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
            last_checkpoint = checkpoints[-1]
            checkpoint_step = last_checkpoint.name.split("-")[-1]
            
            print(f"‚úì –ù–∞–π–¥–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã: {len(checkpoints)} —à—Ç.")
            print(f"‚úì –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: {last_checkpoint.name}")
            print(f"‚úì –û–ë–£–ß–ï–ù–ò–ï –ë–£–î–ï–¢ –í–û–ó–û–ë–ù–û–í–õ–ï–ù–û –° –®–ê–ì–ê {checkpoint_step}")
            print(f"\n–ß–µ–∫–ø–æ–∏–Ω—Ç—ã:")
            for cp in checkpoints[-5:]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5
                print(f"  ‚Ä¢ {cp.name}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            completion_marker = output_dir / "TRAINING_COMPLETED.txt"
            if completion_marker.exists():
                print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª TRAINING_COMPLETED.txt")
                print(f"   –û–±—É—á–µ–Ω–∏–µ —É–∂–µ –±—ã–ª–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —Ä–∞–Ω–µ–µ.")
                print(f"   –î–ª—è –Ω–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —É–¥–∞–ª–∏—Ç–µ –ø–∞–ø–∫—É: {output_dir}")
        else:
            print("‚Ä¢ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            print("‚úì –û–ë–£–ß–ï–ù–ò–ï –ù–ê–ß–ù–Å–¢–°–Ø –° –ù–£–õ–Ø (—ç–ø–æ—Ö–∞ 1, —à–∞–≥ 0)")
    else:
        print("‚Ä¢ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        print("‚úì –û–ë–£–ß–ï–ù–ò–ï –ù–ê–ß–ù–Å–¢–°–Ø –° –ù–£–õ–Ø (—ç–ø–æ—Ö–∞ 1, —à–∞–≥ 0)")
    
    print("=" * 70 + "\n")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    if not data_path.exists():
        print(f"–û–®–ò–ë–ö–ê: –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        return
    
    # –®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    print("\n" + "=" * 70)
    print("–®–ê–ì 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞")
    print("=" * 70)
    device, device_type = get_device()
    torch_dtype = get_torch_dtype(device_type)
    
    # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("\n" + "=" * 70)
    print("–®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
    print("=" * 70)
    tokenizer, model = load_model_and_tokenizer(device, torch_dtype)
    
    # –®–∞–≥ 3: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
    print("\n" + "=" * 70)
    print("–®–ê–ì 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤")
    print("=" * 70)
    model = setup_lora(model)
    
    # –®–∞–≥ 4: –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\n" + "=" * 70)
    print("–®–ê–ì 4: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print("=" * 70)
    dataset = load_training_dataset(data_path)
    
    # –®–∞–≥ 5: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\n" + "=" * 70)
    print("–®–ê–ì 5: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print("=" * 70)
    tokenized_dataset = prepare_dataset(dataset, tokenizer)
    
    # –®–∞–≥ 6: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    print("\n" + "=" * 70)
    print("–®–ê–ì 6: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 70)
    training_args = setup_training_arguments(device_type, output_dir)
    
    # –®–∞–≥ 7: –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n" + "=" * 70)
    print("–®–ê–ì 7: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    print("=" * 70)
    model = train_model(model, tokenizer, tokenized_dataset, training_args)
    
    # –®–∞–≥ 8: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã
    print("\n" + "=" * 70)
    print("–®–ê–ì 8: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤")
    print("=" * 70)
    save_lora_adapters(model, tokenizer, adapters_dir)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    end_time = datetime.now()
    training_duration = end_time - start_time
    hours, remainder = divmod(training_duration.total_seconds(), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    # –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª-–º–∞—Ä–∫–µ—Ä –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    completion_marker = output_dir / "TRAINING_COMPLETED.txt"
    with open(completion_marker, 'w', encoding='utf-8') as f:
        f.write(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {int(hours)}—á {int(minutes)}–º {int(seconds)}—Å\n")
        f.write(f"\nLoRA –∞–¥–∞–ø—Ç–µ—Ä—ã: {adapters_dir}\n")
        f.write(f"–ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {output_dir}\n")
    
    # –í—ã–≤–æ–¥–∏–º –∑–∞–º–µ—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    print("\n" + "üéâ" * 35)
    print("\n" + " " * 15 + "–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print(" " * 10 + "TRAINING COMPLETED SUCCESSFULLY!")
    print("\n" + "üéâ" * 35)
    print("\n" + "=" * 70)
    print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ó–ê–í–ï–†–®–ï–ù–ò–ò")
    print("=" * 70)
    print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞:     {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"–í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è:  {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:     {int(hours)}—á {int(minutes)}–º {int(seconds)}—Å")
    print("=" * 70)
    print("\nüìÅ –°–û–•–†–ê–ù–Å–ù–ù–´–ï –§–ê–ô–õ–´:")
    print(f"   ‚Ä¢ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã: {adapters_dir}")
    print(f"   ‚Ä¢ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {output_dir}")
    print(f"   ‚Ä¢ –ú–∞—Ä–∫–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {completion_marker}")
    print("\nüìñ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:")
    print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
    print("   –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –∞–¥–∞–ø—Ç–µ—Ä—ã —á–µ—Ä–µ–∑ PeftModel.from_pretrained()")
    print("\n" + "=" * 70)
    
    # –ó–≤—É–∫–æ–≤–æ–π —Å–∏–≥–Ω–∞–ª (–¥–ª—è Windows)
    try:
        import winsound
        for _ in range(3):
            winsound.Beep(1000, 500)  # –ß–∞—Å—Ç–æ—Ç–∞ 1000Hz, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å 500–º—Å
    except:
        pass  # –ù–∞ –¥—Ä—É–≥–∏—Ö –û–° –∑–≤—É–∫–æ–≤–æ–π —Å–∏–≥–Ω–∞–ª –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç


if __name__ == "__main__":
    main()

