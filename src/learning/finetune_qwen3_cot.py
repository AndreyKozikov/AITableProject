"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è fine-tuning –º–æ–¥–µ–ª–∏ Qwen3 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Chain-of-Thought reasoning.

–†–∞—Å—à–∏—Ä—è–µ—Ç –±–∞–∑–æ–≤—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª finetune_qwen3.py –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫–∏
—Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –≥—Ä–∞—Ñ–æ–º –∑–Ω–∞–Ω–∏–π.
"""

import os
import json
from pathlib import Path
from datetime import datetime

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model

from src.utils.config import MODEL_CACHE_DIR
from .knowledge_graph import knowledge_graph
from .cot_reasoning import cot_generator


def get_device():
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: TPU ‚Üí GPU ‚Üí CPU."""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ TPU
    try:
        import torch_xla.core.xla_model as xm
        device = xm.xla_device()
        num_cores = os.environ.get("TPU_NUM_CORES", "8")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º TPU (—è–¥—Ä–∞: {num_cores})")
        return device, "tpu"
    except ImportError:
        pass
    except Exception as e:
        print(f"TPU –Ω–∞–π–¥–µ–Ω, –Ω–æ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
    if torch.cuda.is_available():
        print("GPU –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º CUDA")
        return torch.device("cuda"), "gpu"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU
    print("–ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    return torch.device("cpu"), "cpu"


def get_torch_dtype(device_type):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö torch –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
    if device_type == "tpu":
        return torch.bfloat16
    elif device_type == "gpu":
        return torch.float16
    else:
        return torch.float32


def load_model_and_tokenizer(device, torch_dtype):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {MODEL_CACHE_DIR}")
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}, dtype: {torch_dtype}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            str(MODEL_CACHE_DIR),
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            str(MODEL_CACHE_DIR),
            torch_dtype=torch_dtype,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        model = model.to(device)
        
        print("–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        return tokenizer, model
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        print(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_CACHE_DIR}")
        raise


def setup_lora(model):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏."""
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
    
    peft_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    print("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
    return model


def load_training_dataset(data_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ JSONL —Ñ–∞–π–ª–∞."""
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {data_path}")
    
    dataset = load_dataset("json", data_files=str(data_path), split="train")
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
    return dataset


def tokenize_cot_function(example, idx, tokenizer):
    """
    –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–∏–º–µ—Ä —Å Chain-of-Thought reasoning.
    
    –†–∞—Å—à–∏—Ä—è–µ—Ç –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ü–µ–ø–æ—á–µ–∫ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.
    """
    mode = example.get("mode", "unknown")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º assistant –∏–∑ dict –≤ JSON —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ —ç—Ç–æ dict
    assistant_text = example['assistant']
    if isinstance(assistant_text, dict):
        assistant_text = json.dumps(assistant_text, ensure_ascii=False, separators=(',', ':'))
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º system prompt —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
    system_prompt = f"–†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: {mode}. {example['system']}"
    
    # –®–∞–≥ 1: –§–æ—Ä–º–∏—Ä—É–µ–º prompt_text (system + user) —Å generation prompt
    prompt_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": example['user']}
    ]
    prompt_text = tokenizer.apply_chat_template(
        prompt_messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # –®–∞–≥ 2: –§–æ—Ä–º–∏—Ä—É–µ–º full_text (system + user + assistant) –±–µ–∑ generation prompt
    full_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": example['user']},
        {"role": "assistant", "content": assistant_text}
    ]
    full_text = tokenizer.apply_chat_template(
        full_messages,
        tokenize=False,
        add_generation_prompt=False
    )
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ 100-–≥–æ –ø—Ä–æ–º–ø—Ç–∞
    prompt_num = idx + 1
    if prompt_num == 1 or prompt_num % 100 == 0:
        print(f"\n{'='*70}")
        print(f"CoT –ü–†–û–ú–ü–¢ #{prompt_num} (—Ä–µ–∂–∏–º: {mode})")
        print(f"{'='*70}")
        print(f"Prompt —á–∞—Å—Ç—å:\n{prompt_text}")
        print(f"\nFull text:\n{full_text}")
        print(f"{'='*70}\n")
    
    # –®–∞–≥ 3: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –ë–ï–ó truncation –∏ –ë–ï–ó padding
    tokenized = tokenizer(
        full_text,
        truncation=False,
        padding=False
    )
    
    # –®–∞–≥ 4: –°–æ–∑–¥–∞–µ–º labels - –∫–æ–ø–∏—è input_ids
    labels = tokenized["input_ids"].copy()
    
    # –®–∞–≥ 5: –ú–∞—Å–∫–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ labels (system + user)
    prompt_tokenized = tokenizer(prompt_text, add_special_tokens=False)
    prompt_len = len(prompt_tokenized["input_ids"])
    
    # –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã prompt –Ω–∞ -100 (–∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è –≤ loss)
    labels[:prompt_len] = [-100] * prompt_len
    
    tokenized["labels"] = labels
    
    return tokenized


def prepare_dataset(dataset, tokenizer):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∫ –æ–±—É—á–µ–Ω–∏—é - —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã."""
    print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å CoT reasoning...")
    
    tokenized_dataset = dataset.map(
        lambda x, idx: tokenize_cot_function(x, idx, tokenizer),
        with_indices=True,
        remove_columns=["mode", "system", "user", "assistant"]
    )
    
    print("–î–∞—Ç–∞—Å–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω")
    return tokenized_dataset


def setup_training_arguments(device_type, output_dir):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è."""
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è...")
    
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=2e-5,
        num_train_epochs=3,
        logging_dir=str(output_dir / "logs"),
        logging_steps=10,
        save_strategy="steps",
        save_steps=5,
        save_total_limit=2,
        report_to="none",
        remove_unused_columns=False,
        fp16=True if device_type == "gpu" else False,
        bf16=True if device_type == "tpu" else False,
        warmup_steps=100,
        weight_decay=0.01,
        dataloader_num_workers=0,
    )
    
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã:")
    print(f"  - Batch size: 1 (–æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∑–∞ —Ä–∞–∑)")
    print(f"  - Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"  - Learning rate: {training_args.learning_rate}")
    print(f"  - Epochs: {training_args.num_train_epochs}")
    print(f"  - –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: –∫–∞–∂–¥—ã–µ {training_args.save_steps} —à–∞–≥–æ–≤")
    print(f"  - –•—Ä–∞–Ω–∏—Ç—Å—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤: {training_args.save_total_limit}")
    
    return training_args


def train_model(model, tokenizer, tokenized_dataset, training_args, resume_from_checkpoint=None):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Trainer...")
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )
    
    print("=" * 70)
    print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø –° CHAIN-OF-THOUGHT REASONING")
    print("=" * 70)
    
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    
    print("=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 70)
    
    return model


def save_lora_adapters(model, tokenizer, output_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä."""
    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –≤: {output_path}")
    
    output_path.mkdir(parents=True, exist_ok=True)
    
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    print("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    start_time = datetime.now()
    
    print("=" * 70)
    print("FINE-TUNING –ú–û–î–ï–õ–ò QWEN –° LORA + CHAIN-OF-THOUGHT")
    print("=" * 70)
    print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    script_dir = Path(__file__).parent
    data_path = script_dir / "qwen3_cot_structured_train.jsonl"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å –º–æ–¥–µ–ª—å—é
    model_base_dir = MODEL_CACHE_DIR
    output_dir = model_base_dir / "checkpoints_cot"
    adapters_dir = model_base_dir / "lora_adapters_cot"
    
    print(f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_CACHE_DIR}")
    print(f"–î–∞–Ω–Ω—ã–µ: {data_path}")
    print(f"–ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {output_dir}")
    print(f"–ê–¥–∞–ø—Ç–µ—Ä—ã: {adapters_dir}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö
    if not data_path.exists():
        print(f"–û–®–ò–ë–ö–ê: –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_path}")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ create_cot_training_dataset.py")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
    print("\n" + "=" * 70)
    print("–ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ì–†–ê–§–ê –ó–ù–ê–ù–ò–ô")
    print("=" * 70)
    kg_file = script_dir / "knowledge_graph.json"
    if kg_file.exists():
        knowledge_graph.load_from_file(kg_file)
        print(f"‚úì –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {kg_file}")
    else:
        knowledge_graph.save_to_file(kg_file)
        print(f"‚úì –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {kg_file}")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π
    print(f"‚úì –°—É—â–Ω–æ—Å—Ç–µ–π –≤ –≥—Ä–∞—Ñ–µ: {len(knowledge_graph.entities)}")
    print(f"‚úì –°–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ: {len(knowledge_graph.relations)}")
    print(f"‚úì –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–æ–≤: {len(knowledge_graph.get_catalog_manufacturers())}")
    print(f"‚úì –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ç–∏–ø–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: {len(knowledge_graph.get_tool_type_patterns())}")
    
    # –®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    print("\n" + "=" * 70)
    print("–®–ê–ì 1: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞")
    print("=" * 70)
    device, device_type = get_device()
    torch_dtype = get_torch_dtype(device_type)
    
    # –®–∞–≥ 2: –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("\n" + "=" * 70)
    print("–®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
    print("=" * 70)
    tokenizer, model = load_model_and_tokenizer(device, torch_dtype)
    
    # –®–∞–≥ 3: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
    print("\n" + "=" * 70)
    print("–®–ê–ì 3: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤")
    print("=" * 70)
    model = setup_lora(model)
    
    # –®–∞–≥ 4: –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\n" + "=" * 70)
    print("–®–ê–ì 4: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print("=" * 70)
    dataset = load_training_dataset(data_path)
    
    # –®–∞–≥ 5: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\n" + "=" * 70)
    print("–®–ê–ì 5: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å CoT")
    print("=" * 70)
    tokenized_dataset = prepare_dataset(dataset, tokenizer)
    
    # –®–∞–≥ 6: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
    print("\n" + "=" * 70)
    print("–®–ê–ì 6: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 70)
    training_args = setup_training_arguments(device_type, output_dir)
    
    # –®–∞–≥ 7: –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n" + "=" * 70)
    print("–®–ê–ì 7: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å CoT reasoning")
    print("=" * 70)
    model = train_model(model, tokenizer, tokenized_dataset, training_args)
    
    # –®–∞–≥ 8: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã
    print("\n" + "=" * 70)
    print("–®–ê–ì 8: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤")
    print("=" * 70)
    save_lora_adapters(model, tokenizer, adapters_dir)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    end_time = datetime.now()
    training_duration = end_time - start_time
    hours, remainder = divmod(training_duration.total_seconds(), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    # –°–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª-–º–∞—Ä–∫–µ—Ä –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    completion_marker = output_dir / "TRAINING_COMPLETED_COT.txt"
    with open(completion_marker, 'w', encoding='utf-8') as f:
        f.write(f"–û–±—É—á–µ–Ω–∏–µ —Å CoT –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {int(hours)}—á {int(minutes)}–º {int(seconds)}—Å\n")
        f.write(f"\nLoRA –∞–¥–∞–ø—Ç–µ—Ä—ã: {adapters_dir}\n")
        f.write(f"–ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {output_dir}\n")
        f.write(f"–ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π: {kg_file}\n")
    
    # –í—ã–≤–æ–¥–∏–º –∑–∞–º–µ—Ç–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
    print("\n" + "üéâ" * 35)
    print("\n" + " " * 15 + "–û–ë–£–ß–ï–ù–ò–ï –° CoT –ó–ê–í–ï–†–®–ï–ù–û!")
    print(" " * 10 + "TRAINING WITH CHAIN-OF-THOUGHT COMPLETED!")
    print("\n" + "üéâ" * 35)
    print("\n" + "=" * 70)
    print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ó–ê–í–ï–†–®–ï–ù–ò–ò")
    print("=" * 70)
    print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞:     {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"–í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è:  {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:     {int(hours)}—á {int(minutes)}–º {int(seconds)}—Å")
    print("=" * 70)
    print("\nüìÅ –°–û–•–†–ê–ù–Å–ù–ù–´–ï –§–ê–ô–õ–´:")
    print(f"   ‚Ä¢ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã: {adapters_dir}")
    print(f"   ‚Ä¢ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {output_dir}")
    print(f"   ‚Ä¢ –ì—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π: {kg_file}")
    print(f"   ‚Ä¢ –ú–∞—Ä–∫–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {completion_marker}")
    print("\nüìñ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï:")
    print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
    print("   –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ –∞–¥–∞–ø—Ç–µ—Ä—ã —á–µ—Ä–µ–∑ PeftModel.from_pretrained()")
    print("\n" + "=" * 70)


if __name__ == "__main__":
    main()
