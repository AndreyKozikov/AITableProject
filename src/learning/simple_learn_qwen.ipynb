{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-07T05:24:27.390934Z",
     "start_time": "2025-09-07T05:24:10.132688Z"
    }
   },
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForVision2Seq, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Andrew\\GeekBrains\\Python\\AITableProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T05:24:29.597643Z",
     "start_time": "2025-09-07T05:24:29.594010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_device():\n",
    "    \"\"\"Определение устройства: TPU → GPU → CPU\"\"\"\n",
    "    if _has_tpu:\n",
    "        try:\n",
    "            device = xm.xla_device()\n",
    "            num_cores = os.environ.get(\"TPU_NUM_CORES\", \"8\")\n",
    "            print(f\"Используем TPU (ядра: {num_cores})\")\n",
    "            return device, \"tpu\"\n",
    "        except Exception as e:\n",
    "            print(\"TPU найден, но ошибка при инициализации:\", e)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU доступен, используем CUDA\")\n",
    "        return torch.device(\"cuda\"), \"gpu\"\n",
    "\n",
    "    print(\"Используем CPU\")\n",
    "    return torch.device(\"cpu\"), \"cpu\""
   ],
   "id": "c2e183f2eff5ad17",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T05:24:32.661672Z",
     "start_time": "2025-09-07T05:24:32.658384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TPU-библиотека\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    _has_tpu = True\n",
    "except ImportError:\n",
    "    _has_tpu = False"
   ],
   "id": "86b67cba20523a13",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T05:24:34.477057Z",
     "start_time": "2025-09-07T05:24:34.472920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---- выбор устройства ----\n",
    "device, device_type = get_device()\n",
    "\n",
    "# ---- dtype ----\n",
    "if device_type == \"tpu\":\n",
    "    torch_dtype = torch.bfloat16   # только bf16\n",
    "elif device_type == \"gpu\":\n",
    "    torch_dtype = torch.float16    # на GPU можно fp16\n",
    "else:\n",
    "    torch_dtype = torch.float32    # на CPU обычный float32"
   ],
   "id": "5d7e98da9491d38c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используем CPU\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T05:24:43.455746Z",
     "start_time": "2025-09-07T05:24:38.610230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Модель и токенизатор. Преобразовываем текст (слова) в токены\n",
    "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained( #AutoModelForVision2Seq автоматический класс из библиотеки transformers, который подбирает правильный токенизатор для данной модели, from_pretrained загружает готовый токенизатор по имени модели\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True\n",
    ").to(device)"
   ],
   "id": "5121958c4a8f56fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Andrew\\GeekBrains\\Python\\AITableProject\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2214: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T05:24:49.339225Z",
     "start_time": "2025-09-07T05:24:49.282037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Добавляем LoRA (Low-Rank Adaptation) техника дообучения больших моделей с малым числом параметров\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # Ранг матриц Чем больше r, тем больше добавляется обучаемых параметров и тем выше точность адаптации, но больше нагрузка на память.\n",
    "    lora_alpha=32, # Контролирует, насколько сильно добавочные веса (из LoRA) влияют на модель. регулирует «усиление» новых параметров\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Целевые слои для Qwen. Слои модели, куда будут вставлены LoRA-адаптеры\n",
    "    lora_dropout=0.05, #dropout некоторых связей во время обучения\n",
    "    bias=\"none\", # Определяет, будут ли дообучаться смещения (bias terms). \"all\" или \"lora_only\"\n",
    "    task_type=\"CAUSAL_LM\" #Тип задачи. \"CAUSAL_LM\" = причинно-следственная языковая модель (autoregressive language model, т.е. предсказание следующего токена)\n",
    ")\n",
    "model = get_peft_model(model, peft_config) # Оборачивает исходную модель в оболочку PEFT (Parameter-Efficient Fine-Tuning). В выбранные модули (q_proj, v_proj) вставляются LoRA-адаптеры. Обучаются только\n",
    "# LoRA-параметры, а остальные веса модели остаются замороженными.\n",
    "model.print_trainable_parameters()  # Показывает, сколько параметров будут обучаться"
   ],
   "id": "ebddec116a0755f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,179,072 || all params: 2,211,164,672 || trainable%: 0.0985\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Загружаем наш датасет\n",
    "data_files = \"./datasets/data/train.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")"
   ],
   "id": "d6a0169c012ecba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "В transformers при обучении языковых моделей (Causal LM)\n",
    "\n",
    "input_ids → токены, которые подаются на вход модели.\n",
    "\n",
    "labels → эталонные ответы, которые модель должна предсказывать.\n",
    "\n",
    "Модель обучается так:\n",
    "На каждом шаге она получает часть input_ids и должна предсказать следующий токен из labels.\n",
    "\n",
    "Если мы обучаем авторегрессию (CAUSAL LM), то:\n",
    "\n",
    "вход (input_ids) и эталон (labels) совпадают.\n",
    "\n",
    "Разница в том, что внутри модели логиты на позиции i используются для предсказания токена на позиции i+1.\n",
    "\n",
    "Но часто нам нужно, чтобы модель училась только на ответе, а не на самом вопросе.\n",
    "Для этого делают маскирование промпта в labels:\n",
    "\n",
    "Все токены промпта заменяют на -100.\n",
    "\n",
    "В transformers это специальное значение, означающее «не учитывать в лоссе».\n",
    "\n",
    "\n",
    "Для нашей задачи\n",
    "\n",
    "Формат входа и выхода должен быть строго определён.\n",
    "\n",
    "Лучше всего представлять таблицу в текстовом виде:\n",
    "\n",
    "вход: строка с разделителями,\n",
    "\n",
    "выход: та же строка, но с новыми колонками.\n",
    "\n",
    "labels должны обучать модель именно на генерацию выходной таблицы, а не на повторение входа.\n",
    "\n",
    "То есть labels должны покрывать только output, а prompt (т.е. сам «вопрос») нужно замаскировать (-100 в labels)."
   ],
   "id": "cb10e4f3915dfc1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize(example):\n",
    "    # Формируем промпт: даём задачу модели\n",
    "    prompt = f\"Преобразуй данные:\\n{json.dumps(example['input'], ensure_ascii=False)}\\n\\nВыход (только JSON):\\n:\\n\"\n",
    "\n",
    "    # Выход приводим к строке JSON\n",
    "    output = json.dumps(example[\"output\"], ensure_ascii=False)\n",
    "\n",
    "    # Соединяем в один текст\n",
    "    full_text = prompt + output\n",
    "\n",
    "    # Токенизируем весь текст (и промпт, и ответ)\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # labels = копия input_ids\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    # Длина промпта (чтобы не считать его в лоссе)\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"input\", \"output\"])"
   ],
   "id": "6d8009aa200c69b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Параметры обучения\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1 if device_type != \"gpu\" else 2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True if device_type == \"gpu\" else False,\n",
    "    bf16=True if device_type == \"tpu\" else False,\n",
    "    tpu_num_cores=1 if device_type == \"tpu\" else None\n",
    ")"
   ],
   "id": "6900582678d15bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# collator будет автоматически выравнивать длину примеров в батче\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "b16fbf4047f5db20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Сохраняем только адаптеры LoRA\n",
    "model.save_pretrained(\"./qwen-lora-adapters\")\n",
    "tokenizer.save_pretrained(\"./qwen-lora-adapters\")"
   ],
   "id": "d7617b7274a0d59f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T05:43:55.522972Z",
     "start_time": "2025-09-07T05:42:46.671614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Для инференса: Загружаем базовую модель и добавляем адаптеры\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "import json\n",
    "\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./qwen-lora-adapters\",\n",
    "    torch_dtype=torch_dtype,\n",
    ").to(device)\n",
    "\n",
    "test_input = \"Отвертка 7018-2025 ГОСТ 1050-2023;Шт.;4140715;15\"\n",
    "test_input_list = test_input.split(\";\")\n",
    "\n",
    "prompt = f\"\"\"Преобразуй данные:\\n{json.dumps(test_input_list, ensure_ascii=False)}\\nВыход (только JSON):\\n\n",
    "\"\"\"\n",
    "\n",
    "# ---- настройка пайплайна ----\n",
    "if device_type == \"gpu\":\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0   # GPU\n",
    "    )\n",
    "    output = pipe(prompt, max_new_tokens=200)\n",
    "    print(\"==== Сырой вывод ====\")\n",
    "    print(output)\n",
    "    print(\"\\n==== Только ответ ====\")\n",
    "    print(output[0][\"generated_text\"].strip())\n",
    "\n",
    "elif device_type == \"cpu\":\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=-1  # CPU\n",
    "    )\n",
    "    output = pipe(prompt, max_new_tokens=200)\n",
    "    print(\"==== Сырой вывод ====\")\n",
    "    print(output)\n",
    "    print(\"\\n==== Только ответ ====\")\n",
    "    print(output[0][\"generated_text\"].strip())\n",
    "\n",
    "elif device_type == \"tpu\":\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ],
   "id": "3b60821a767b980",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Сырой вывод ====\n",
      "[{'generated_text': 'Преобразуй данные:\\n[\"Отвертка 7018-2025 ГОСТ 1050-2023\", \"Шт.\", \"4140715\", \"15\"]\\nВыход (только JSON):\\n\\n {\"Наименование\": \"Отвертка 7018-2025 ГОСТ 1050-2023\", \"Единица измерения\": \"шт.\", \"Количество\": \"4140715\", \"Техническое задание\": \"Артикул: 1050000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'}]\n",
      "\n",
      "==== Только ответ ====\n",
      "Преобразуй данные:\n",
      "[\"Отвертка 7018-2025 ГОСТ 1050-2023\", \"Шт.\", \"4140715\", \"15\"]\n",
      "Выход (только JSON):\n",
      "\n",
      " {\"Наименование\": \"Отвертка 7018-2025 ГОСТ 1050-2023\", \"Единица измерения\": \"шт.\", \"Количество\": \"4140715\", \"Техническое задание\": \"Артикул: 1050000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aeeea0d36c0a6c22",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
